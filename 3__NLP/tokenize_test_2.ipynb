{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b456dd46-54f9-4491-b83c-9a3c8794e6f2",
   "metadata": {},
   "source": [
    "2. Sentence Tokenize\n",
    "\n",
    "Tokenising based on sentence requires you to split on the period ('.'). Lets use nltk sentense tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbcf018-9e00-4e88-a4c8-cebbd302520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d3d366-2d76-4518-8f3d-cdf830751699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "doc = document.split()\n",
    "print(doc)\n",
    "#-split function concatinates the \".\" so we should use word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763fc2a4-4d49-487e-b811-a13bcb89760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenising based on sentence requires you to split on the period ('.'). Lets use nltk sentense tokeniser.\n",
    "\n",
    "from nltk import word_tokenize\n",
    "words = word_tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c30687f-23cc-45bd-a9f0-9fbb1de5275a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"At nine o'clock I visited him myself.\",\n",
       " \"It looks like religious mania, and he'll soon think that he himself is God.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(document)\n",
    "sentences # this gives the list of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b913d7-ccc3-46ef-b917-356026ce055e",
   "metadata": {},
   "source": [
    "### 3. Tweet tokeniser\n",
    "\n",
    "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff6ec30a-dc9a-4689-a75d-75dc43319e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'ðŸ˜', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'ðŸ˜Ž']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it ðŸ˜. it was gr8 <3. #bingewatching #nothingtodo ðŸ˜Ž\"\n",
    "words = word_tokenize(message)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c802dd-dab5-47ee-aeeb-bc1de312ba7e",
   "metadata": {},
   "source": [
    "Words doesn't concatinate the \"<3\" smily. We use Tweet tokenizer to fix this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0431958-1a1b-4250-aa6e-018b0ae9481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':)', '.', 'i', 'totally', 'loved', 'it', 'ðŸ˜', '.', 'it', 'was', 'gr8', '<3', '.', '#bingewatching', '#nothingtodo', 'ðŸ˜Ž']\n"
     ]
    }
   ],
   "source": [
    "from nltk import TweetTokenizer\n",
    "print(TweetTokenizer().tokenize(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b87d2e-88c7-4ec7-9b07-07588f4e16f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
