{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894a46f3-1f8e-4361-843b-54e560fcf6bd",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "1. Word Tokenisation\n",
    "2. Sentence Tokenisation\n",
    "3. Tweet Tokentisation\n",
    "4. Custom tokenisation using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83322c-5d0d-4c84-9a2c-b0b17e548dd6",
   "metadata": {},
   "source": [
    "### 1. Word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a48bd83-a0f6-4c76-8002-40cf0c25add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8cdd8f6-d4d8-4193-95c4-819df28f8a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "print(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c69815-329f-420d-84e3-239c899d1121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(document)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de32e6d-69df-4a3e-8144-44557951e162",
   "metadata": {},
   "source": [
    "### 2. Sentence tokeniser\n",
    "Tokenising based on sentence requires you to split on the period ('.'). Let's use nltk sentence tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c346432f-5f2e-447f-a27a-c4eb64340d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(document)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedc388-0567-4282-9365-1e400e545c49",
   "metadata": {},
   "source": [
    "### 3. Tweet tokeniser\n",
    "\n",
    "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44191ddb-e162-441c-b1eb-e934b0f1483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "words = word_tokenize(message)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a276c4c-7c32-45c8-a54c-1f8e60a5af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':)', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<3', '.', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "print(TweetTokenizer().tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2958f8b-8949-4904-b550-8dd6db75f6b4",
   "metadata": {},
   "source": [
    "## 4. Custom Tokeinser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b5b5b2f-9573-468b-9edf-392e382aa095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "regexp_tokenize(message, \"#[\\w]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a48d1-a996-496c-af66-ae1609ebf2f7",
   "metadata": {},
   "source": [
    "## 1. Assignment\n",
    "Write a piece of code that breaks a given sentence into words and stores them in a list. Then remove the stop words from this list and then print the list as well as the length of the list. Again, use the NLTK tokeniser to do this.\n",
    "\n",
    "Sample input: <br> \n",
    "‚ÄúEducation is the most powerful weapon that you can use to change the world‚Äù\n",
    "\n",
    "## 2. Assignment\n",
    "Write a piece of code that breaks a given sentence into words and store them in a list. Then print the list as well as the length of the list. Use the NLTK tokeniser to tokenise words.\n",
    "\n",
    "Sample input:\n",
    "\"I love pasta\"\n",
    "\n",
    "## 3. Assignment\n",
    "Write a Python code using the NLTK library that breaks a given piece of text containing multiple sentences into different sentences. Finally print the total number of sentences in the text.\n",
    "\n",
    "Sample input: \n",
    "Develop a passion for your learning. If you do, you‚Äôll never cease to grow.\n",
    "\n",
    "## 4. Assignment\n",
    "Use NLTK‚Äôs regex tokeniser to extract all the mentions from a given tweet and then print the total number of mentions. A mention comprises of a ‚Äò@‚Äô symbol followed by a username containing either alphabets, numbers or underscores.\n",
    "\n",
    "Sample tweet:\n",
    "So excited to be a part of machine learning and artificial intelligence program made by @MIT and @iiitb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
